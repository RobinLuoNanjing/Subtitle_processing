1
00:00:00,400 --> 00:00:01,739
 在本段视频中 我想告诉大家

2
00:00:01,739 --> 00:00:05,133
 一些关于梯度下降算法的实用技巧

3
00:00:05,133 --> 00:00:06,563
 我将集中讨论

4
00:00:06,563 --> 00:00:09,414
 学习率 α

5
00:00:09,551 --> 00:00:11,625
 具体来说 这是梯度下降算法的

6
00:00:11,640 --> 00:00:13,677
 更新规则

7
00:00:13,677 --> 00:00:14,908
 这里我想要

8
00:00:14,908 --> 00:00:16,784
 告诉大家

9
00:00:16,784 --> 00:00:18,629
 如何调试

10
00:00:18,629 --> 00:00:19,994
 也就是我认为应该如何确定

11
00:00:19,994 --> 00:00:22,385
 梯度下降是正常工作的

12
00:00:22,390 --> 00:00:23,632
 此外我还想告诉大家

13
00:00:23,632 --> 00:00:25,879
 如何选择学习率 α

14
00:00:25,890 --> 00:00:27,079
 也就是我平常

15
00:00:27,079 --> 00:00:29,222
 如何选择这个参数

16
00:00:29,222 --> 00:00:30,702
 我通常是怎样确定

17
00:00:30,702 --> 00:00:34,125
 梯度下降正常工作的

18
00:00:34,125 --> 00:00:35,852
 梯度下降算法所做的事情

19
00:00:35,852 --> 00:00:37,107
 就是为你找到

20
00:00:37,107 --> 00:00:38,710
 一个 θ 值

21
00:00:38,710 --> 00:00:42,692
 并希望它能够最小化代价函数 J(θ)

22
00:00:42,692 --> 00:00:44,285
 我通常会在

23
00:00:44,300 --> 00:00:46,121
 梯度下降算法运行时

24
00:00:46,121 --> 00:00:49,731
 绘出代价函数 J(θ) 的值

25
00:00:49,750 --> 00:00:51,367
 这里的 x 轴是表示

26
00:00:51,367 --> 00:00:52,828
 梯度下降算法的

27
00:00:52,850 --> 00:00:54,278
 迭代步数

28
00:00:54,278 --> 00:00:55,985
 你可能会得到

29
00:00:55,985 --> 00:00:59,722
 这样一条曲线

30
00:00:59,722 --> 00:01:01,249
 注意 这里的 x 轴

31
00:01:01,249 --> 00:01:03,592
 是迭代步数

32
00:01:03,592 --> 00:01:05,098
 在我们以前看到的

33
00:01:05,098 --> 00:01:07,050
 J(θ) 曲线中

34
00:01:07,050 --> 00:01:08,931
 x 轴 也就是横轴

35
00:01:08,950 --> 00:01:13,122
 曾经用来表示参数 θ 但这里不是

36
00:01:13,122 --> 00:01:15,068
 具体来说

37
00:01:15,090 --> 00:01:17,725
 这一点的含义是这样的

38
00:01:17,725 --> 00:01:20,608
 当我运行完100步的梯度下降迭代之后

39
00:01:20,608 --> 00:01:22,608
 无论我得到

40
00:01:22,620 --> 00:01:24,095
 什么 θ 值

41
00:01:24,110 --> 00:01:25,620
 总之 100步迭代之后

42
00:01:25,620 --> 00:01:27,139
 我将得到

43
00:01:27,150 --> 00:01:29,150
 一个 θ 值

44
00:01:29,150 --> 00:01:30,683
 根据100步迭代之后

45
00:01:30,683 --> 00:01:32,857
 得到的这个 θ 值

46
00:01:32,857 --> 00:01:34,118
 我将算出

47
00:01:34,120 --> 00:01:36,272
 代价函数 J(θ) 的值

48
00:01:36,272 --> 00:01:37,718
 而这个点的垂直高度就代表

49
00:01:37,718 --> 00:01:39,961
 梯度下降算法

50
00:01:39,961 --> 00:01:41,135
 100步迭代之后

51
00:01:41,135 --> 00:01:42,212
 得到的 θ

52
00:01:42,220 --> 00:01:44,050
 算出的 J(θ) 值

53
00:01:44,050 --> 00:01:46,515
 而这个点

54
00:01:46,515 --> 00:01:48,253
 则是梯度下降算法

55
00:01:48,253 --> 00:01:50,120
 迭代200次之后

56
00:01:50,120 --> 00:01:52,097
 得到的 θ

57
00:01:52,097 --> 00:01:55,172
 算出的 J(θ) 值

58
00:01:55,172 --> 00:01:56,713
 所以这条曲线

59
00:01:56,720 --> 00:01:58,177
 显示的是

60
00:01:58,200 --> 00:02:02,025
 梯度下降算法迭代过程中代价函数 J(θ) 的值

61
00:02:02,025 --> 00:02:03,335
 如果梯度下降算法

62
00:02:03,350 --> 00:02:05,180
 正常工作

63
00:02:05,190 --> 00:02:08,952
 那么每一步迭代之后

64
00:02:08,952 --> 00:02:12,219
 J(θ) 都应该下降

65
00:02:16,451 --> 00:02:19,248
 这条曲线

66
00:02:19,248 --> 00:02:20,580
 的一个用处在于

67
00:02:20,580 --> 00:02:22,545
 它可以告诉你

68
00:02:22,545 --> 00:02:24,147
 如果你看一下

69
00:02:24,160 --> 00:02:26,015
 我画的这条曲线

70
00:02:26,030 --> 00:02:27,581
 当你达到

71
00:02:27,581 --> 00:02:29,744
 300步迭代之后

72
00:02:29,744 --> 00:02:31,348
 也就是300步到400步迭代之间

73
00:02:31,348 --> 00:02:32,908
 也就是曲线的这一段

74
00:02:32,910 --> 00:02:35,792
 看起来 J(θ) 并没有下降多少

75
00:02:35,810 --> 00:02:36,930
 所以当你

76
00:02:36,960 --> 00:02:38,785
 到达400步迭代时

77
00:02:38,810 --> 00:02:41,554
 这条曲线看起来已经很平坦了

78
00:02:41,554 --> 00:02:43,334
 也就是说

79
00:02:43,340 --> 00:02:44,533
 在这里400步迭代的时候

80
00:02:44,533 --> 00:02:45,847
 梯度下降算法

81
00:02:45,850 --> 00:02:47,868
 基本上已经收敛了

82
00:02:47,880 --> 00:02:50,493
 因为代价函数并没有继续下降

83
00:02:50,493 --> 00:02:51,625
 所以说 看这条曲线

84
00:02:51,625 --> 00:02:53,390
 可以帮助你判断

85
00:02:53,420 --> 00:02:56,812
 梯度下降算法是否已经收敛

86
00:02:57,550 --> 00:02:58,900
 顺便说一下

87
00:02:58,900 --> 00:03:00,820
 对于每一个特定的问题

88
00:03:00,820 --> 00:03:02,098
 梯度下降算法所需的迭代次数

89
00:03:02,098 --> 00:03:04,251
 可以相差很大

90
00:03:04,251 --> 00:03:06,092
 也许对于某一个问题

91
00:03:06,130 --> 00:03:07,832
 梯度下降算法

92
00:03:07,832 --> 00:03:10,198
 只需要30步迭代就可以收敛

93
00:03:10,210 --> 00:03:12,670
 然而换一个问题

94
00:03:12,670 --> 00:03:15,042
 也许梯度下降算法就需要3000步迭代

95
00:03:15,050 --> 00:03:17,996
 对于另一个机器学习问题

96
00:03:17,996 --> 00:03:19,823
 则可能需要三百万步迭代

97
00:03:19,823 --> 00:03:20,768
 实际上

98
00:03:20,768 --> 00:03:22,312
 我们很难提前判断

99
00:03:22,312 --> 00:03:24,333
 梯度下降算法

100
00:03:24,360 --> 00:03:26,252
 需要多少步迭代才能收敛

101
00:03:26,252 --> 00:03:28,935
 通常我们需要画出这类曲线

102
00:03:28,940 --> 00:03:32,958
 画出代价函数随迭代步数数增加的变化曲线

103
00:03:32,960 --> 00:03:34,347
 通常 我会通过看这种曲线

104
00:03:34,347 --> 00:03:35,610
 来试着判断

105
00:03:35,610 --> 00:03:38,470
 梯度下降算法是否已经收敛

106
00:03:38,590 --> 00:03:40,118
 另外 也可以

107
00:03:40,130 --> 00:03:42,748
 进行一些自动的收敛测试

108
00:03:42,748 --> 00:03:44,306
 也就是说用一种算法

109
00:03:44,306 --> 00:03:46,593
 来告诉你梯度下降算法

110
00:03:46,593 --> 00:03:48,615
 是否已经收敛

111
00:03:48,620 --> 00:03:50,268
 自动收敛测试

112
00:03:50,268 --> 00:03:52,505
 一个非常典型的例子是

113
00:03:52,540 --> 00:03:54,981
 如果代价函数 J(θ)

114
00:03:54,981 --> 00:03:57,009
 的下降小于

115
00:03:57,020 --> 00:03:58,396
 一个很小的值 ε

116
00:03:58,396 --> 00:04:01,435
 那么就认为已经收敛

117
00:04:01,435 --> 00:04:02,412
 比如可以选择

118
00:04:02,412 --> 00:04:05,272
 1e-3

119
00:04:05,272 --> 00:04:07,065
 但我发现

120
00:04:07,070 --> 00:04:10,740
 通常要选择一个合适的阈值 ε 是相当困难的

121
00:04:10,740 --> 00:04:12,049
 因此 为了检查

122
00:04:12,049 --> 00:04:14,058
 梯度下降算法是否收敛

123
00:04:14,090 --> 00:04:15,361
 我实际上还是

124
00:04:15,361 --> 00:04:17,074
 通过看

125
00:04:17,074 --> 00:04:18,299
 左边的这条曲线图

126
00:04:18,310 --> 00:04:21,778
 而不是依靠自动收敛测试

127
00:04:21,778 --> 00:04:22,778
 此外 这种曲线图

128
00:04:22,780 --> 00:04:24,340
 也可以

129
00:04:24,340 --> 00:04:25,812
 在算法没有正常工作时

130
00:04:25,820 --> 00:04:28,659
 提前警告你

131
00:04:28,690 --> 00:04:30,185
 具体地说

132
00:04:30,200 --> 00:04:31,641
 如果代价函数 J(θ)

133
00:04:31,650 --> 00:04:34,850
 随迭代步数

134
00:04:34,850 --> 00:04:35,864
 的变化曲线是这个样子

135
00:04:35,864 --> 00:04:37,105
 J(θ) 实际上在不断上升

136
00:04:37,120 --> 00:04:39,136
 那么这就很明确的表示

137
00:04:39,136 --> 00:04:42,898
 梯度下降算法没有正常工作

138
00:04:42,898 --> 00:04:44,552
 而这样的曲线图

139
00:04:44,552 --> 00:04:48,253
 通常意味着你应该使用较小的学习率 α

140
00:04:48,270 --> 00:04:49,697
 如果 J(θ) 在上升

141
00:04:49,697 --> 00:04:51,564
 那么最常见的原因是

142
00:04:51,580 --> 00:04:53,199
 你在最小化

143
00:04:53,199 --> 00:04:54,904
 这样的

144
00:04:54,904 --> 00:04:59,346
 一个函数

145
00:04:59,346 --> 00:05:00,518
 这时如果你的学习率太大

146
00:05:00,518 --> 00:05:01,614
 当你从这里开始

147
00:05:01,614 --> 00:05:03,202
 梯度下降算法

148
00:05:03,202 --> 00:05:05,516
 可能将冲过最小值达到这里

149
00:05:05,516 --> 00:05:07,145
 而如果你的学习率太大

150
00:05:07,145 --> 00:05:08,473
 你可能再次冲过最小值

151
00:05:08,500 --> 00:05:10,493
 达到这里

152
00:05:10,500 --> 00:05:12,279
 然后一直这样下去

153
00:05:12,279 --> 00:05:13,810
 而你真正想要的是

154
00:05:13,810 --> 00:05:17,991
 从这里开始慢慢的下降

155
00:05:17,991 --> 00:05:19,465
 但是 如果学习率过大

156
00:05:19,465 --> 00:05:21,252
 那么梯度下降算法

157
00:05:21,252 --> 00:05:22,749
 将会不断的

158
00:05:22,760 --> 00:05:24,454
 冲过最小值

159
00:05:24,454 --> 00:05:26,147
 然后你将得到

160
00:05:26,160 --> 00:05:27,170
 越来越糟糕的结果

161
00:05:27,210 --> 00:05:28,720
 得到越来越大的

162
00:05:28,780 --> 00:05:30,744
 代价函数 J(θ) 值

163
00:05:30,744 --> 00:05:31,751
 所以如果你得到了

164
00:05:31,751 --> 00:05:33,263
 这样一个曲线图

165
00:05:33,263 --> 00:05:34,248
 如果你看到这样一个曲线图

166
00:05:34,248 --> 00:05:36,106
 通常的解决方法是

167
00:05:36,106 --> 00:05:38,182
 使用较小的 α 值

168
00:05:38,182 --> 00:05:39,640
 当然也要确保

169
00:05:39,790 --> 00:05:41,872
 你的代码中没有错误

170
00:05:41,872 --> 00:05:43,268
 但通常最可能

171
00:05:43,268 --> 00:05:44,709
 出现的错误是

172
00:05:44,709 --> 00:05:48,105
 α 值过大

173
00:05:49,050 --> 00:05:50,595
 同样的 有时你可能

174
00:05:50,595 --> 00:05:52,115
 看到这种形状的

175
00:05:52,120 --> 00:05:53,188
 J(θ) 曲线

176
00:05:53,188 --> 00:05:54,158
 它先下降 然后上升

177
00:05:54,160 --> 00:05:56,325
 接着又下降 然后又上升

178
00:05:56,330 --> 00:05:57,366
 然后再次下降

179
00:05:57,366 --> 00:05:58,910
 再次上升 如此往复

180
00:05:58,930 --> 00:06:00,150
 而解决这种情况的方法

181
00:06:00,150 --> 00:06:04,052
 通常同样是选择较小 α 值

182
00:06:04,090 --> 00:06:05,129
 我不打算证明这一点

183
00:06:05,129 --> 00:06:07,128
 但对于我们讨论的线性回归

184
00:06:07,128 --> 00:06:10,806
 可以很容易从数学上证明

185
00:06:10,830 --> 00:06:12,608
 只要学习率足够小

186
00:06:12,608 --> 00:06:13,913
 那么每次迭代之后

187
00:06:13,913 --> 00:06:15,885
 代价函数 J(θ)

188
00:06:15,885 --> 00:06:19,025
 都会下降

189
00:06:19,030 --> 00:06:21,342
 因此如果代价函数没有下降

190
00:06:21,342 --> 00:06:22,338
 那可能以为着学习率过大

191
00:06:22,338 --> 00:06:23,992
 这时你就应该尝试一个较小的学习率

192
00:06:23,992 --> 00:06:24,867
 当然 你也不希望

193
00:06:24,890 --> 00:06:25,788
 学习度太小

194
00:06:25,788 --> 00:06:27,068
 因为如果这样

195
00:06:27,070 --> 00:06:28,095
 如果你这么做

196
00:06:28,095 --> 00:06:31,543
 那么梯度下降算法可能收敛得很慢

197
00:06:31,543 --> 00:06:32,812
 如果学习率 α 太小

198
00:06:32,812 --> 00:06:34,804
 你可能

199
00:06:34,804 --> 00:06:36,945
 从这里开始

200
00:06:36,960 --> 00:06:38,248
 然后很缓慢很缓慢

201
00:06:38,248 --> 00:06:40,408
 向最低点移动

202
00:06:40,408 --> 00:06:41,338
 这样一来

203
00:06:41,338 --> 00:06:42,974
 你需要迭代很多次

204
00:06:42,980 --> 00:06:47,064
 才能到达最低点

205
00:06:47,090 --> 00:06:48,118
 因此 如果学习率 α 太小

206
00:06:48,118 --> 00:06:49,500
 梯度下降算法

207
00:06:49,570 --> 00:06:52,989
 的收敛将会很缓慢

208
00:06:53,005 --> 00:06:55,377
 总结一下

209
00:06:55,377 --> 00:06:57,301
 如果学习率 α 太小

210
00:06:57,301 --> 00:06:59,672
 你会遇到收敛速度慢的问题

211
00:06:59,672 --> 00:07:01,161
 而如果学习率 α 太大

212
00:07:01,161 --> 00:07:02,494
 代价函数 J(θ) 可能不会在

213
00:07:02,494 --> 00:07:04,378
 每次迭代都下降

214
00:07:04,378 --> 00:07:06,023
 甚至可能不收敛

215
00:07:06,023 --> 00:07:08,579
 在某些情况下

216
00:07:08,579 --> 00:07:10,957
 如果学习率 α 过大

217
00:07:10,990 --> 00:07:14,710
 也可能出现收敛缓慢的问题

218
00:07:14,800 --> 00:07:16,312
 但更常见的情况是

219
00:07:16,312 --> 00:07:17,380
 你会发现代价函数 J(θ)

220
00:07:17,440 --> 00:07:20,532
 并不会在每次迭代之后都下降

221
00:07:20,540 --> 00:07:22,223
 而为了调试

222
00:07:22,223 --> 00:07:24,539
 所有这些情况

223
00:07:24,539 --> 00:07:26,053
 绘制J(θ)随迭代步数变化的曲线

224
00:07:26,070 --> 00:07:29,315
 通常可以帮助你弄清楚到底发生了什么

225
00:07:29,315 --> 00:07:31,258
 具体来说

226
00:07:31,258 --> 00:07:32,525
 当我运行梯度下降算法时

227
00:07:32,525 --> 00:07:34,997
 我通常会尝试一系列α值

228
00:07:35,000 --> 00:07:36,555
 所以在运行梯度下降算法制

229
00:07:36,580 --> 00:07:37,988
 请尝试不同的 α 值

230
00:07:37,988 --> 00:07:39,902
 比如0.001, 0.01

231
00:07:39,902 --> 00:07:41,471
 这里每隔10倍

232
00:07:41,471 --> 00:07:43,275
 取一个值

233
00:07:43,280 --> 00:07:44,449
 然后对于这些不同的 α 值

234
00:07:44,449 --> 00:07:45,769
 绘制 J(θ)

235
00:07:45,769 --> 00:07:47,015
 随迭代步数变化的曲线

236
00:07:47,030 --> 00:07:49,202
 然后选择

237
00:07:49,202 --> 00:07:51,094
 看上去使得 J(θ)

238
00:07:51,094 --> 00:07:54,805
 快速下降的一个 α 值

239
00:07:54,805 --> 00:07:58,067
 事实上 我通常并不是隔10倍取一个值

240
00:07:58,067 --> 00:07:59,305
 你可以看到

241
00:07:59,305 --> 00:08:01,780
 这里是每隔10倍取一个值

242
00:08:01,869 --> 00:08:03,860
 我通常取的

243
00:08:03,870 --> 00:08:07,164
 是这些 α 值

244
00:08:07,164 --> 00:08:09,770
 一直这样下去

245
00:08:09,816 --> 00:08:11,365
 你看 先取0.001

246
00:08:11,365 --> 00:08:13,598
 然后将学习率增加3倍

247
00:08:13,598 --> 00:08:15,182
 得到0.003

248
00:08:15,182 --> 00:08:17,356
 然后这一步

249
00:08:17,356 --> 00:08:20,187
 从0.003到0.01

250
00:08:20,187 --> 00:08:22,145
 又大约增加了3倍

251
00:08:22,145 --> 00:08:24,605
 所以 在为梯度下降算法

252
00:08:24,605 --> 00:08:27,343
 选择合适的学习率时

253
00:08:27,343 --> 00:08:28,580
 我大致是

254
00:08:28,580 --> 00:08:30,592
 按3的倍数来取值的

255
00:08:30,592 --> 00:08:32,256
 所以我会尝试一系列α值

256
00:08:32,256 --> 00:08:33,495
 直到我找到

257
00:08:33,495 --> 00:08:34,725
 一个值

258
00:08:34,725 --> 00:08:35,757
 它不能再小了

259
00:08:35,757 --> 00:08:37,137
 同时找到另一个值

260
00:08:37,137 --> 00:08:38,394
 它不能再大了

261
00:08:38,394 --> 00:08:40,560
 然后我尽量挑选

262
00:08:40,560 --> 00:08:42,279
 其中最大的那个 α 值

263
00:08:42,279 --> 00:08:43,764
 或者一个比最大值

264
00:08:43,764 --> 00:08:46,488
 略小一些的合理的值

265
00:08:46,488 --> 00:08:48,178
 而当我做了以上工作时

266
00:08:48,178 --> 00:08:49,592
 我通常就可以得到

267
00:08:49,592 --> 00:08:51,968
 一个不错的学习率

268
00:08:51,968 --> 00:08:53,203
 如果也你这样做

269
00:08:53,203 --> 00:08:54,345
 那么你也能够

270
00:08:54,345 --> 00:08:55,906
 为你的梯度下降算法

271
00:08:55,906 --> 00:08:57,340
 找到一个合适的

272
00:08:57,340 --> 00:08:58,563
 学习率值 【教育无边界字幕组】翻译：王祖超 校对：所罗门捷列夫 审核：Naplessss
